{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_readme.py\n",
    "# -----------------------------------------------------------------------------\n",
    "# GÃ©nÃ¨re/actualise README.md Ã  partir de reports/metrics.json\n",
    "# - GÃ¨re 2 schÃ©mas :\n",
    "#   1) {\"train\": {...}}  -> rÃ©sumÃ© train\n",
    "#   2) {\"ModA\": {...}, \"ModB\": {...}} -> tableau comparatif des modÃ¨les (TEST)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "METRICS_PATH = Path(\"reports/metrics.json\")\n",
    "FIG_DIR = Path(\"reports/figures\")\n",
    "README_PATH = Path(\"README.md\")\n",
    "\n",
    "def _fmt(x, nd=3):\n",
    "    try:\n",
    "        return f\"{float(x):.{nd}f}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "def _load_metrics():\n",
    "    if not METRICS_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Fichier introuvable: {METRICS_PATH}\")\n",
    "    with open(METRICS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def _detect_schema(data: dict):\n",
    "    \"\"\"\n",
    "    Retourne \"train\" si data contient la clÃ© 'train',\n",
    "    sinon \"models\" si c'est un mapping {model -> metrics}.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict) and \"train\" in data and isinstance(data[\"train\"], dict):\n",
    "        return \"train\"\n",
    "    # Heuristique: toutes les valeurs sont des dicts avec au moins pr_auc/roc_auc\n",
    "    if isinstance(data, dict) and data:\n",
    "        vals = list(data.values())\n",
    "        if all(isinstance(v, dict) for v in vals):\n",
    "            return \"models\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def _render_train_section(train: dict) -> str:\n",
    "    rows = []\n",
    "    rows.append(\"| MÃ©trique | Valeur |\")\n",
    "    rows.append(\"|---|---|\")\n",
    "    rows.append(f\"| PR-AUC | {_fmt(train.get('pr_auc'))} |\")\n",
    "    rows.append(f\"| ROC-AUC | {_fmt(train.get('roc_auc'))} |\")\n",
    "    rows.append(f\"| Precision (classe 1) | {_fmt(train.get('precision'))} |\")\n",
    "    rows.append(f\"| Recall (classe 1) | {_fmt(train.get('recall'))} |\")\n",
    "    rows.append(f\"| F1 (classe 1) | {_fmt(train.get('f1'))} |\")\n",
    "    thr = train.get(\"threshold\", None)\n",
    "    if thr is not None:\n",
    "        rows.append(f\"| Seuil optimal (FÎ²=2) | {_fmt(thr)} |\")\n",
    "    md = \"\\n\".join(rows)\n",
    "    return f\"\"\"## ðŸ“Š RÃ©sultats (train)\n",
    "\n",
    "*(donnÃ©es extraites de `reports/metrics.json`)*\n",
    "\n",
    "{md}\n",
    "\n",
    "> Le seuil est calibrÃ© par **FÎ²=2** (priorise le rappel).\n",
    "\"\"\"\n",
    "\n",
    "def _render_models_table(models: dict) -> str:\n",
    "    # Trie par PR-AUC desc puis Recall desc\n",
    "    items = []\n",
    "    for name, m in models.items():\n",
    "        items.append({\n",
    "            \"model\": name,\n",
    "            \"pr_auc\": m.get(\"pr_auc\"),\n",
    "            \"roc_auc\": m.get(\"roc_auc\"),\n",
    "            \"precision\": m.get(\"precision\"),\n",
    "            \"recall\": m.get(\"recall\"),\n",
    "            \"f1\": m.get(\"f1\"),\n",
    "            \"threshold\": m.get(\"threshold\"),\n",
    "        })\n",
    "    items = sorted(items, key=lambda d: (d.get(\"pr_auc\") or 0.0, d.get(\"recall\") or 0.0), reverse=True)\n",
    "\n",
    "    # DÃ©termine le meilleur (PR-AUC max)\n",
    "    best = items[0] if items else None\n",
    "    best_line = \"\"\n",
    "    if best:\n",
    "        best_line = (\n",
    "            f\"> ðŸ… **Meilleur modÃ¨le (PR-AUC)** : **{best['model']}**  \\n\"\n",
    "            f\"> Scores â€” PR-AUC: {_fmt(best['pr_auc'])} | ROC-AUC: {_fmt(best['roc_auc'])} | \"\n",
    "            f\"Precision: {_fmt(best['precision'])} | Recall: {_fmt(best['recall'])} | F1: {_fmt(best['f1'])}\"\n",
    "        )\n",
    "\n",
    "    # Tableau Markdown\n",
    "    rows = []\n",
    "    rows.append(\"| ModÃ¨le | PR-AUC | ROC-AUC | Precision | Recall | F1 | Seuil |\")\n",
    "    rows.append(\"|---|---:|---:|---:|---:|---:|---:|\")\n",
    "    for d in items:\n",
    "        rows.append(\n",
    "            f\"| {d['model']} | {_fmt(d['pr_auc'])} | {_fmt(d['roc_auc'])} | \"\n",
    "            f\"{_fmt(d['precision'])} | {_fmt(d['recall'])} | {_fmt(d['f1'])} | {_fmt(d['threshold'])} |\"\n",
    "        )\n",
    "    table = \"\\n\".join(rows)\n",
    "\n",
    "    return f\"\"\"## ðŸ§  Comparaison des modÃ¨les (test)\n",
    "\n",
    "*(donnÃ©es extraites de `reports/metrics.json`)*\n",
    "\n",
    "{table}\n",
    "\n",
    "{best_line}\n",
    "\"\"\"\n",
    "\n",
    "def _figures_section() -> str:\n",
    "    figs = []\n",
    "    if FIG_DIR.exists():\n",
    "        for f in sorted(FIG_DIR.glob(\"*.png\")):\n",
    "            figs.append(f\"- `{f.as_posix()}`\")\n",
    "    flist = \"\\n\".join(figs) if figs else \"_(Aucune figure trouvÃ©e dans `reports/figures/`)_\"\n",
    "    return f\"\"\"## ðŸ“ˆ Visualisations\n",
    "\n",
    "Les figures exportÃ©es (si prÃ©sentes) :\n",
    "\n",
    "{flist}\n",
    "\"\"\"\n",
    "\n",
    "def _project_header() -> str:\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return f\"\"\"# ðŸ’³ DÃ©tection de Fraude â€” Projet\n",
    "\n",
    "> **DerniÃ¨re mise Ã  jour automatique :** {ts}\n",
    "\n",
    "Ce projet dÃ©tecte des transactions bancaires frauduleuses Ã  partir du dataset public\n",
    "**Credit Card Fraud Detection (Kaggle)**.\n",
    "\"\"\"\n",
    "\n",
    "def _project_structure() -> str:\n",
    "    return \"\"\"## ðŸ“‚ Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tech_section() -> str:\n",
    "    return \"\"\"## ðŸ§© Stack / Outils\n",
    "\n",
    "- Python (pandas, numpy, scikit-learn, imbalanced-learn, xgboost)\n",
    "- MÃ©triques : **PR-AUC**, **ROC-AUC**, **Recall**, **Precision**, **F1**\n",
    "- Ã‰quilibrage : **SMOTE**\n",
    "- Optimisation : **GridSearchCV** (CV stratifiÃ©e, scoring = *average_precision*)\n",
    "\"\"\"\n",
    "\n",
    "def _footer() -> str:\n",
    "    return \"\"\"---\n",
    "\n",
    "*Ce README est gÃ©nÃ©rÃ© automatiquement Ã  partir de `reports/metrics.json`.*\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    data = _load_metrics()\n",
    "    schema = _detect_schema(data)\n",
    "\n",
    "    md = [_project_header(), _project_structure(), _tech_section()]\n",
    "\n",
    "    if schema == \"train\":\n",
    "        md.append(_render_train_section(data[\"train\"]))\n",
    "    elif schema == \"models\":\n",
    "        md.append(_render_models_table(data))\n",
    "    else:\n",
    "        md.append(\"âš ï¸ `reports/metrics.json` dÃ©tectÃ© mais schÃ©ma non reconnu. Attendu: `{'train': {...}}` ou `{model: metrics}`.\")\n",
    "\n",
    "    md.append(_figures_section())\n",
    "    md.append(_footer())\n",
    "\n",
    "    README_PATH.write_text(\"\\n\\n\".join(md), encoding=\"utf-8\")\n",
    "    print(f\"[OK] README mis Ã  jour -> {README_PATH.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(\"[ERREUR]\", e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
